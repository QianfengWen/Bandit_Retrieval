{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "class Dataloader(ABC):    \n",
    "    @abstractmethod\n",
    "    def load_dataset(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_questions(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_passages(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_relevance_map(self):\n",
    "        pass\n",
    "\n",
    "    def load_data(self):\n",
    "        self.dataset = self.load_dataset()\n",
    "        question_ids, question_texts = self.load_questions()\n",
    "        passage_ids, passage_texts = self.load_passages()\n",
    "        relevance_map = self.create_relevance_map()\n",
    "        \n",
    "        return question_ids, question_texts, passage_ids, passage_texts, relevance_map\n",
    "\n",
    "    def visualize_relevance(self, fig_path):\n",
    "        relevance = [qrel.relevance for qrel in self.dataset.qrels_iter()]\n",
    "        plt.hist(relevance, bins=len(set(relevance)))\n",
    "        plt.xlabel('Relevance')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Relevance Distribution')\n",
    "        plt.savefig(fig_path)\n",
    "        return\n",
    "    \n",
    "    def save_relevance_map(self, data_dir, relevance_map):\n",
    "        data_path = os.path.join(data_dir, \"relevance_map.json\")\n",
    "        with open(data_path, 'w') as json_file:\n",
    "            json.dump(relevance_map, json_file, indent=4)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import ir_datasets\n",
    "\n",
    "class Scidocs(Dataloader):\n",
    "    def load_dataset(self):\n",
    "        dataset = ir_datasets.load(\"beir/scidocs\")\n",
    "        return dataset\n",
    "    \n",
    "    def load_questions(self):\n",
    "        # avoid duplicate questions\n",
    "        question_map = dict()\n",
    "        sub = dict()\n",
    "\n",
    "        for query in self.dataset.queries_iter():\n",
    "            if query.text in question_map:\n",
    "                sub[query.query_id] = question_map[query.text]\n",
    "            else:\n",
    "                question_map[query.text] = query.query_id\n",
    "            \n",
    "        self.question_sub = sub\n",
    "\n",
    "        question_ids = list(question_map.values())\n",
    "        question_texts = list(question_map.keys())\n",
    "\n",
    "        return question_ids, question_texts\n",
    "\n",
    "\n",
    "    def load_passages(self):\n",
    "        # avoid duplicate passages\n",
    "        passage_map = dict()\n",
    "        sub = dict()\n",
    "\n",
    "        for passage in self.dataset.docs_iter():\n",
    "            if passage.text in passage_map:\n",
    "                sub[passage.doc_id] = passage_map[passage.text]\n",
    "            else:\n",
    "                passage_map[passage.text] = passage.doc_id\n",
    "\n",
    "        self.passage_sub = sub\n",
    "\n",
    "        passage_ids = list(passage_map.values())\n",
    "        passage_texts = list(passage_map.keys())\n",
    "\n",
    "        return passage_ids, passage_texts\n",
    "    \n",
    "\n",
    "    def create_relevance_map(self):\n",
    "        relevance_map = defaultdict(dict)\n",
    "        for qrel in self.dataset.qrels_iter():\n",
    "            query_id = qrel.query_id\n",
    "            doc_id = qrel.doc_id\n",
    "            relevance = qrel.relevance\n",
    "            \n",
    "            if relevance <= 0:\n",
    "                continue\n",
    "\n",
    "            if query_id in self.question_sub:\n",
    "                query_id = self.question_sub[query_id]\n",
    "            \n",
    "            if doc_id in self.passage_sub:\n",
    "                doc_id = self.passage_sub[doc_id]\n",
    "            \n",
    "            relevance_map[query_id][doc_id] = relevance\n",
    "        \n",
    "        return relevance_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scidocs = Scidocs()\n",
    "question_ids, question_texts, passage_ids, passage_texts, relevance_map = scidocs.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_id_to_idx = {question_ids[i]: i for i in range(len(question_ids))}\n",
    "passage_id_to_idx = {passage_ids[i]: i for i in range(len(passage_ids))}\n",
    "question_idx_to_id = {i: question_ids[i] for i in range(len(question_ids))}\n",
    "passage_idx_to_id = {i: passage_ids[i] for i in range(len(passage_ids))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing embeddings...\n",
      "Loading Sentence Transformer model: all-MiniLM-L6-v2\n",
      "Computing embeddings for 1000 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b511a041b4224556b0de068c32e0cb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentence Transformer model: all-MiniLM-L6-v2\n",
      "Computing embeddings for 25265 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d22f4688a8840cd9d29e7711efd15b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question embeddings shape: (1000, 384)\n",
      "Passage embeddings shape: (25265, 384)\n"
     ]
    }
   ],
   "source": [
    "## Pre-compute Embeddings with Sentence Transformer\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def precompute_embeddings(texts, model_name='all-MiniLM-L6-v2', batch_size=32):\n",
    "    \"\"\"\n",
    "    Precompute embeddings for a list of texts using Sentence Transformer.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to embed\n",
    "        model_name: Name of the sentence transformer model to use\n",
    "        batch_size: Batch size for embedding computation\n",
    "        \n",
    "    Returns:\n",
    "        numpy array of embeddings\n",
    "    \"\"\"\n",
    "    print(f\"Loading Sentence Transformer model: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Compute embeddings in batches\n",
    "    print(f\"Computing embeddings for {len(texts)} texts...\")\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch_embeddings = model.encode(batch, show_progress_bar=False)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Precompute question and passage embeddings\n",
    "model_name = 'all-MiniLM-L6-v2'  # You can change this to other models like 'paraphrase-mpnet-base-v2' for better quality\n",
    "print(\"Pre-computing embeddings...\")\n",
    "\n",
    "question_embeddings = precompute_embeddings(question_texts, model_name=model_name)\n",
    "passage_embeddings = precompute_embeddings(passage_texts, model_name=model_name)\n",
    "\n",
    "print(f\"Question embeddings shape: {question_embeddings.shape}\")\n",
    "print(f\"Passage embeddings shape: {passage_embeddings.shape}\")\n",
    "\n",
    "# Save embeddings (optional)\n",
    "import os\n",
    "if not os.path.exists('embeddings'):\n",
    "    os.makedirs('embeddings')\n",
    "    \n",
    "np.save('embeddings/question_embeddings.npy', question_embeddings)\n",
    "np.save('embeddings/passage_embeddings.npy', passage_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Bandit Retrieval with GP-UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, Matern\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class LLMInterface:\n",
    "    \"\"\"Interface for querying LLM models for relevance scoring.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt-3.5-turbo\", api_key=None, temperature=0.0, delay=0.0,\n",
    "                question_embeddings=None, passage_embeddings=None):\n",
    "        \"\"\"\n",
    "        Initialize the LLM interface.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the LLM model to use\n",
    "            api_key: API key for the LLM service (if applicable)\n",
    "            temperature: Temperature parameter for LLM sampling\n",
    "            delay: Simulated delay in seconds (for testing without actual LLM calls)\n",
    "            question_embeddings: Pre-computed embeddings for questions\n",
    "            passage_embeddings: Pre-computed embeddings for passages\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key\n",
    "        self.temperature = temperature\n",
    "        self.delay = delay\n",
    "        self.call_count = 0\n",
    "        \n",
    "        # Store precomputed embeddings\n",
    "        self.question_embeddings = question_embeddings\n",
    "        self.passage_embeddings = passage_embeddings\n",
    "        self.has_precomputed_embeddings = (question_embeddings is not None and \n",
    "                                          passage_embeddings is not None)\n",
    "        \n",
    "        # For simulation purposes\n",
    "        self.is_simulation = True\n",
    "        self.sim_scores = {}\n",
    "    \n",
    "    def get_relevance_score(self, query, passage, relevance_map=None, query_id=None, passage_id=None,\n",
    "                           query_idx=None, passage_idx=None):\n",
    "        \"\"\"\n",
    "        Get relevance score for a query-passage pair from the LLM.\n",
    "        \n",
    "        In a real implementation, this would query the actual LLM.\n",
    "        For demonstration purposes, we'll simulate responses based on either:\n",
    "        1. The actual relevance scores if available in relevance_map\n",
    "        2. A simulated score based on cosine similarity plus noise\n",
    "        \n",
    "        Args:\n",
    "            query: The query text\n",
    "            passage: The passage text\n",
    "            relevance_map: Optional ground truth relevance map for simulation\n",
    "            query_id: ID of the query in the relevance map\n",
    "            passage_id: ID of the passage in the relevance map\n",
    "            query_idx: Index of the query in precomputed embeddings\n",
    "            passage_idx: Index of the passage in precomputed embeddings\n",
    "            \n",
    "        Returns:\n",
    "            float: Relevance score between 0 and 1\n",
    "        \"\"\"\n",
    "        self.call_count += 1\n",
    "        \n",
    "        if self.delay > 0:\n",
    "            time.sleep(self.delay)  # Simulate LLM API latency\n",
    "        \n",
    "        # For simulation purposes\n",
    "        if self.is_simulation:\n",
    "            # Use ground truth if available\n",
    "            if relevance_map and query_id and passage_id:\n",
    "                if query_id in relevance_map and passage_id in relevance_map[query_id]:\n",
    "                    # Scale the relevance to [0, 1]\n",
    "                    true_rel = relevance_map[query_id][passage_id]\n",
    "                    # Perfect retrieval\n",
    "                    return true_rel\n",
    "            \n",
    "            # Generate key for caching\n",
    "            key = f\"{query}_{passage}\"\n",
    "            if key in self.sim_scores:\n",
    "                return self.sim_scores[key]\n",
    "            \n",
    "            # Simulate relevance with cosine similarity + noise using precomputed embeddings\n",
    "            if self.has_precomputed_embeddings and query_idx is not None and passage_idx is not None:\n",
    "                # Use precomputed embeddings directly\n",
    "                query_embedding = self.question_embeddings[query_idx]\n",
    "                passage_embedding = self.passage_embeddings[passage_idx]\n",
    "                \n",
    "                # Calculate cosine similarity directly using numpy dot product\n",
    "                # This avoids the sklearn reshape issue\n",
    "                similarity = np.dot(query_embedding, passage_embedding) / (\n",
    "                    np.linalg.norm(query_embedding) * np.linalg.norm(passage_embedding)\n",
    "                )\n",
    "            else:\n",
    "                # Fall back to computing embeddings on the fly if needed\n",
    "                sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "                query_embedding = sentence_model.encode(query)\n",
    "                passage_embedding = sentence_model.encode(passage)\n",
    "                \n",
    "                # Calculate cosine similarity directly\n",
    "                similarity = np.dot(query_embedding, passage_embedding) / (\n",
    "                    np.linalg.norm(query_embedding) * np.linalg.norm(passage_embedding)\n",
    "                )\n",
    "            \n",
    "            # Perfect retrieval\n",
    "            score = similarity\n",
    "            self.sim_scores[key] = score\n",
    "            return score\n",
    "        \n",
    "        else:\n",
    "            # In a real implementation, this would call the actual LLM API\n",
    "            # Placeholder for actual implementation\n",
    "            return random.random()\n",
    "\n",
    "class BanditRetrieval:\n",
    "    \"\"\"Bandit-based retrieval system using GP-UCB for selective LLM querying.\"\"\"\n",
    "    \n",
    "    def __init__(self, questions, passages, query_encoder='tfidf', passage_encoder='tfidf', \n",
    "                 kernel=None, beta=2.0, normalize_features=True, gp_noise=0.1,\n",
    "                 init_samples=5, budget=100, llm=None, \n",
    "                 question_embeddings=None, passage_embeddings=None):\n",
    "        \"\"\"\n",
    "        Initialize the bandit retrieval system.\n",
    "        \n",
    "        Args:\n",
    "            questions: List of question texts\n",
    "            passages: List of passage texts\n",
    "            query_encoder: Method for encoding queries ('tfidf', 'sentence-transformer', etc.)\n",
    "            passage_encoder: Method for encoding passages\n",
    "            kernel: Kernel for Gaussian Process, if None uses default RBF\n",
    "            beta: Exploration parameter for UCB\n",
    "            normalize_features: Whether to normalize the feature vectors\n",
    "            gp_noise: Noise level for Gaussian Process\n",
    "            init_samples: Number of initial samples per query before using GP-UCB\n",
    "            budget: Maximum number of LLM calls per query\n",
    "            llm: LLM interface for retrieving relevance scores\n",
    "            question_embeddings: Pre-computed embeddings for questions (numpy array)\n",
    "            passage_embeddings: Pre-computed embeddings for passages (numpy array)\n",
    "        \"\"\"\n",
    "        self.questions = questions\n",
    "        self.passages = passages\n",
    "        self.query_encoder = query_encoder\n",
    "        self.passage_encoder = passage_encoder\n",
    "        self.beta = beta\n",
    "        self.normalize_features = normalize_features\n",
    "        self.init_samples = init_samples\n",
    "        self.budget = budget\n",
    "        self.llm = llm if llm else LLMInterface()\n",
    "        \n",
    "        # Store pre-computed embeddings\n",
    "        self.question_embeddings = question_embeddings\n",
    "        self.passage_embeddings = passage_embeddings\n",
    "        self.using_precomputed = question_embeddings is not None and passage_embeddings is not None\n",
    "        \n",
    "        # Set default kernel if none provided\n",
    "        if kernel is None:\n",
    "            # Matern kernel is often good for text-based tasks\n",
    "            length_scale = 1.0\n",
    "            self.kernel = ConstantKernel(1.0) * Matern(length_scale=length_scale, nu=1.5)\n",
    "        else:\n",
    "            self.kernel = kernel\n",
    "            \n",
    "        self.gp_noise = gp_noise\n",
    "        \n",
    "        # Initialize encoder based on specified method (only if not using pre-computed embeddings)\n",
    "        if not self.using_precomputed:\n",
    "            self._initialize_encoders()\n",
    "        else:\n",
    "            print(\"Using pre-computed embeddings\")\n",
    "    \n",
    "    def _encode_query_passage_pair(self, query_idx, passage_idx):\n",
    "        \"\"\"\n",
    "        Encode a query-passage pair into a feature vector using pre-computed embeddings.\n",
    "        \n",
    "        Args:\n",
    "            query_idx: Index of the query in self.questions\n",
    "            passage_idx: Index of the passage in self.passages\n",
    "            \n",
    "        Returns:\n",
    "            feature_vector: Combined feature vector representing the query-passage pair\n",
    "        \"\"\"\n",
    "        if self.using_precomputed:\n",
    "            # Use pre-computed embeddings\n",
    "            query_vec = self.question_embeddings[query_idx]\n",
    "            passage_vec = self.passage_embeddings[passage_idx]\n",
    "            \n",
    "            # Combine features - here we use concatenation\n",
    "            feature_vector = np.concatenate([query_vec, passage_vec])\n",
    "            \n",
    "            # Optionally normalize\n",
    "            if self.normalize_features:\n",
    "                norm = np.linalg.norm(feature_vector)\n",
    "                if norm > 0:\n",
    "                    feature_vector = feature_vector / norm\n",
    "                    \n",
    "            return feature_vector\n",
    "        else:\n",
    "            # Fall back to original method for encoding with texts\n",
    "            query = self.questions[query_idx]\n",
    "            passage = self.passages[passage_idx]\n",
    "            \n",
    "            # Encode query\n",
    "            if self.query_encoder == 'tfidf':\n",
    "                query_vec = self.tfidf.transform([query]).toarray().flatten()\n",
    "            elif self.query_encoder == 'sentence-transformer':\n",
    "                query_vec = self.sentence_model.encode(query)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown query encoder: {self.query_encoder}\")\n",
    "                \n",
    "            # Encode passage\n",
    "            if self.passage_encoder == 'tfidf':\n",
    "                passage_vec = self.tfidf.transform([passage]).toarray().flatten()\n",
    "            elif self.passage_encoder == 'sentence-transformer':\n",
    "                passage_vec = self.sentence_model.encode(passage)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown passage encoder: {self.passage_encoder}\")\n",
    "                \n",
    "            # Combine features\n",
    "            feature_vector = np.concatenate([query_vec, passage_vec])\n",
    "            \n",
    "            # Optionally normalize\n",
    "            if self.normalize_features:\n",
    "                norm = np.linalg.norm(feature_vector)\n",
    "                if norm > 0:\n",
    "                    feature_vector = feature_vector / norm\n",
    "                    \n",
    "            return feature_vector\n",
    "        \n",
    "    def _ucb_score(self, means, stds, t):\n",
    "        \"\"\"Calculate UCB scores for a given set of means and standard deviations.\"\"\"\n",
    "        return means + self.beta * stds / np.sqrt(t)\n",
    "    \n",
    "    def retrieve(self, query_idx, query_id=None, relevance_map=None, verbose=True, return_trajectories=False):\n",
    "        \"\"\"\n",
    "        Retrieve relevant passages for a given query using GP-UCB.\n",
    "        \n",
    "        Args:\n",
    "            query_idx: Index of the query in self.questions\n",
    "            query_id: ID of the query (for relevance map lookup)\n",
    "            relevance_map: Map of ground truth relevance scores (for evaluation)\n",
    "            verbose: Whether to print progress information\n",
    "            return_trajectories: Whether to return mean/std trajectories for visualization\n",
    "            \n",
    "        Returns:\n",
    "            ranked_passages: List of passage indices ranked by relevance\n",
    "            scores: Relevance scores for each passage\n",
    "            If return_trajectories is True, also returns lists of means and stds\n",
    "        \"\"\"\n",
    "        query = self.questions[query_idx]\n",
    "        n_passages = len(self.passages)\n",
    "        \n",
    "        # Initialize Gaussian Process Regressor\n",
    "        gp = GaussianProcessRegressor(\n",
    "            kernel=self.kernel,\n",
    "            alpha=self.gp_noise,\n",
    "            normalize_y=True,\n",
    "            n_restarts_optimizer=2\n",
    "        )\n",
    "        \n",
    "        # Data structures to track observations\n",
    "        X_observed = []  # Feature vectors of observed query-passage pairs\n",
    "        y_observed = []  # Observed relevance scores\n",
    "        observed_indices = set()  # Indices of observed passages\n",
    "        \n",
    "        # For tracking and visualization\n",
    "        means_history = []\n",
    "        stds_history = []\n",
    "        \n",
    "        # Initialize by sampling a few passages randomly\n",
    "        initial_indices = np.random.choice(n_passages, min(self.init_samples, n_passages), replace=False)\n",
    "        \n",
    "        for passage_idx in initial_indices:\n",
    "            # Encode the query-passage pair\n",
    "            feature_vector = self._encode_query_passage_pair(query_idx, passage_idx)\n",
    "            \n",
    "            passage_id = passage_idx_to_id[passage_idx]\n",
    "            # print(f\"Passage ID: {passage_id}\")\n",
    "            # print(f\"Query ID: {query_id}\")\n",
    "            # print(f\"Relevance map: {relevance_map[query_id]}\")\n",
    "            \n",
    "            # Get relevance score from LLM\n",
    "            if relevance_map and query_id:\n",
    "                if query_id in relevance_map and passage_id in relevance_map[query_id]:\n",
    "                    score = relevance_map[query_id][passage_id]\n",
    "                else:\n",
    "                    score = 0\n",
    "            else:\n",
    "                score = self.llm.get_relevance_score(query, self.passages[passage_idx], self.question_embeddings[query_idx], self.passage_embeddings[passage_idx])\n",
    "                \n",
    "            # Store observation\n",
    "            X_observed.append(feature_vector)\n",
    "            y_observed.append(score)\n",
    "            observed_indices.add(passage_idx)\n",
    "        \n",
    "        # Main GP-UCB loop\n",
    "        pbar = tqdm(total=self.budget, disable=not verbose)\n",
    "        pbar.update(len(initial_indices))\n",
    "        \n",
    "        while len(observed_indices) < min(self.budget, n_passages):\n",
    "            # Fit GP model to observed data\n",
    "            if len(X_observed) > 0:\n",
    "                X_array = np.vstack(X_observed)\n",
    "                y_array = np.array(y_observed)\n",
    "                gp.fit(X_array, y_array)\n",
    "            \n",
    "            # Encode all unobserved passages and calculate UCB scores\n",
    "            candidate_indices = [i for i in range(n_passages) if i not in observed_indices]\n",
    "            \n",
    "            if not candidate_indices:\n",
    "                break  # All passages have been observed\n",
    "                \n",
    "            X_candidates = []\n",
    "            for passage_idx in candidate_indices:\n",
    "                feature_vector = self._encode_query_passage_pair(query_idx, passage_idx)\n",
    "                X_candidates.append(feature_vector)\n",
    "                \n",
    "            X_candidates = np.vstack(X_candidates)\n",
    "            \n",
    "            # Get posterior mean and standard deviation\n",
    "            means, stds = gp.predict(X_candidates, return_std=True)\n",
    "\n",
    "            print(f\"means: {means}\")\n",
    "            \n",
    "            # Store for visualization\n",
    "            mean_full = np.zeros(n_passages)\n",
    "            std_full = np.zeros(n_passages)\n",
    "            for i, idx in enumerate(candidate_indices):\n",
    "                mean_full[idx] = means[i]\n",
    "                std_full[idx] = stds[i]\n",
    "            for idx in observed_indices:\n",
    "                idx_obs = list(observed_indices).index(idx)\n",
    "                mean_full[idx] = y_observed[idx_obs]\n",
    "                std_full[idx] = 0.0  # No uncertainty for observed points\n",
    "                \n",
    "            means_history.append(mean_full.copy())\n",
    "            stds_history.append(std_full.copy())\n",
    "            \n",
    "            # Calculate UCB scores\n",
    "            t = len(observed_indices) + 1  # Current iteration\n",
    "            ucb_scores = self._ucb_score(means, stds, t)\n",
    "            \n",
    "            # Select passage with highest UCB score\n",
    "            ucb_idx = np.argmax(ucb_scores)\n",
    "            selected_idx = candidate_indices[ucb_idx]\n",
    "            selected_passage = self.passages[selected_idx]\n",
    "            \n",
    "            # Get relevance score from LLM\n",
    "            if relevance_map and query_id:\n",
    "                score = self.llm.get_relevance_score(\n",
    "                    query, selected_passage, relevance_map=relevance_map, \n",
    "                    query_id=query_id, passage_id=selected_idx\n",
    "                )\n",
    "            else:\n",
    "                score = self.llm.get_relevance_score(query, selected_passage)\n",
    "                \n",
    "            # Store observation\n",
    "            X_observed.append(X_candidates[ucb_idx])\n",
    "            y_observed.append(score)\n",
    "            observed_indices.add(selected_idx)\n",
    "            \n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Optional: Early stopping if convergence criteria met\n",
    "            # For example, if the highest standard deviation is below a threshold\n",
    "            if np.max(stds) < 0.05 and len(observed_indices) > self.init_samples:\n",
    "                if verbose:\n",
    "                    print(f\"Stopping early at {len(observed_indices)} observations due to convergence\")\n",
    "                break\n",
    "                \n",
    "        pbar.close()\n",
    "        \n",
    "        # Final prediction for all passages\n",
    "        all_features = []\n",
    "        for passage_idx in range(n_passages):\n",
    "            feature_vector = self._encode_query_passage_pair(query_idx, passage_idx)\n",
    "            all_features.append(feature_vector)\n",
    "            \n",
    "        X_all = np.vstack(all_features)\n",
    "        \n",
    "        # Fit the final GP model\n",
    "        X_array = np.vstack(X_observed)\n",
    "        y_array = np.array(y_observed)\n",
    "        gp.fit(X_array, y_array)\n",
    "        \n",
    "        # Get posterior mean for all passages\n",
    "        final_means, final_stds = gp.predict(X_all, return_std=True)\n",
    "        \n",
    "        # For observed passages, use the actual scores\n",
    "        for i, idx in enumerate(observed_indices):\n",
    "            idx_obs = list(observed_indices).index(idx)\n",
    "            final_means[idx] = y_observed[idx_obs]\n",
    "            \n",
    "        # Rank passages by relevance score\n",
    "        ranked_indices = np.argsort(-final_means)\n",
    "        \n",
    "        if return_trajectories:\n",
    "            return ranked_indices, final_means, means_history, stds_history\n",
    "        else:\n",
    "            return ranked_indices, final_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(ranked_indices, true_relevance, cutoffs=[5, 10, 20, 50, 100]):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval performance using precision, recall, and nDCG at different cutoffs.\n",
    "    \n",
    "    Args:\n",
    "        ranked_indices: List of passage indices ranked by relevance\n",
    "        true_relevance: Dictionary mapping passage indices to relevance scores\n",
    "        cutoffs: List of cutoff values for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        metrics: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Convert true relevance to a list aligned with ranked_indices\n",
    "    true_rel_list = np.zeros(len(ranked_indices))\n",
    "    for idx, score in true_relevance.items():\n",
    "        idx = passage_id_to_idx[idx]\n",
    "        if idx < len(true_rel_list):\n",
    "            true_rel_list[idx] = score\n",
    "    \n",
    "    # Sort true relevance according to ranked_indices\n",
    "    sorted_rel = true_rel_list[ranked_indices]\n",
    "    \n",
    "    for k in cutoffs:\n",
    "        if k > len(ranked_indices):\n",
    "            continue\n",
    "            \n",
    "        # Precision@k\n",
    "        precision = np.sum(sorted_rel[:k] > 0) / k\n",
    "        metrics[f'P@{k}'] = precision\n",
    "        \n",
    "        # Recall@k\n",
    "        total_relevant = np.sum(true_rel_list > 0)\n",
    "        if total_relevant > 0:\n",
    "            recall = np.sum(sorted_rel[:k] > 0) / total_relevant\n",
    "            metrics[f'R@{k}'] = recall\n",
    "        else:\n",
    "            metrics[f'R@{k}'] = 0.0\n",
    "            \n",
    "        # nDCG@k\n",
    "        dcg = np.sum(sorted_rel[:k] / np.log2(np.arange(2, k+2)))\n",
    "        \n",
    "        # Ideal ordering for iDCG\n",
    "        ideal_rel = np.sort(true_rel_list)[::-1]\n",
    "        idcg = np.sum(ideal_rel[:k] / np.log2(np.arange(2, k+2)))\n",
    "        \n",
    "        if idcg > 0:\n",
    "            ndcg = dcg / idcg\n",
    "            metrics[f'nDCG@{k}'] = ndcg\n",
    "        else:\n",
    "            metrics[f'nDCG@{k}'] = 0.0\n",
    "            \n",
    "    return metrics\n",
    "\n",
    "def plot_metrics(metrics_list, query_ids):\n",
    "    \"\"\"Plot average metrics across all queries.\"\"\"\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Combine metrics from all queries\n",
    "    all_metrics = {}\n",
    "    for i, metrics in enumerate(metrics_list):\n",
    "        for k, v in metrics.items():\n",
    "            if k not in all_metrics:\n",
    "                all_metrics[k] = []\n",
    "            all_metrics[k].append(v)\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {k: np.mean(v) for k, v in all_metrics.items()}\n",
    "    \n",
    "    # Extract metric groups\n",
    "    precision_metrics = {k: v for k, v in avg_metrics.items() if k.startswith('P@')}\n",
    "    recall_metrics = {k: v for k, v in avg_metrics.items() if k.startswith('R@')}\n",
    "    ndcg_metrics = {k: v for k, v in avg_metrics.items() if k.startswith('nDCG@')}\n",
    "    \n",
    "    # Create a figure with 3 subplots for each metric type\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot each metric group\n",
    "    for i, (metrics, title) in enumerate(zip(\n",
    "        [precision_metrics, recall_metrics, ndcg_metrics],\n",
    "        ['Precision@k', 'Recall@k', 'nDCG@k']\n",
    "    )):\n",
    "        # Sort by cutoff value\n",
    "        cutoffs = [int(k.split('@')[1]) for k in metrics.keys()]\n",
    "        sorted_idx = np.argsort(cutoffs)\n",
    "        x = [list(metrics.keys())[i] for i in sorted_idx]\n",
    "        y = [list(metrics.values())[i] for i in sorted_idx]\n",
    "        \n",
    "        axes[i].bar(x, y, color='skyblue')\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        for j, value in enumerate(y):\n",
    "            axes[i].text(j, value + 0.02, f'{value:.3f}', ha='center')\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    return fig, avg_metrics\n",
    "\n",
    "def visualize_gp_ucb_trajectory(means_history, stds_history, query, top_passages=5):\n",
    "    \"\"\"Visualize the GP-UCB algorithm's trajectory of means and uncertainties.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    n_iterations = len(means_history)\n",
    "    n_passages = len(means_history[0])\n",
    "    \n",
    "    # Plot mean estimates over iterations\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    # Get top k passages from final iteration\n",
    "    final_means = means_history[-1]\n",
    "    top_k_indices = np.argsort(-final_means)[:top_passages]\n",
    "    \n",
    "    # Plot means for top k passages\n",
    "    for i in top_k_indices:\n",
    "        means = [m[i] for m in means_history]\n",
    "        plt.plot(range(n_iterations), means, '-o', label=f'Passage {i}')\n",
    "        \n",
    "    plt.title(f'Mean Estimates for Top {top_passages} Passages')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Relevance Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot standard deviations over iterations\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i in top_k_indices:\n",
    "        stds = [s[i] for s in stds_history]\n",
    "        plt.plot(range(n_iterations), stds, '-o', label=f'Passage {i}')\n",
    "        \n",
    "    plt.title(f'Standard Deviations for Top {top_passages} Passages')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Uncertainty (σ)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'GP-UCB Trajectory for Query: \"{query[:50]}...\"', y=1.05)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LLMInterface' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create bandit retrieval system with pre-computed embeddings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m LLMInterface(delay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)  \u001b[38;5;66;03m# For simulation\u001b[39;00m\n\u001b[0;32m      3\u001b[0m bandit \u001b[38;5;241m=\u001b[39m BanditRetrieval(\n\u001b[0;32m      4\u001b[0m     questions\u001b[38;5;241m=\u001b[39mquestion_texts,\n\u001b[0;32m      5\u001b[0m     passages\u001b[38;5;241m=\u001b[39mpassage_texts,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     passage_embeddings\u001b[38;5;241m=\u001b[39mpassage_embeddings     \u001b[38;5;66;03m# Pass pre-computed embeddings\u001b[39;00m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Select a few queries for demonstration\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LLMInterface' is not defined"
     ]
    }
   ],
   "source": [
    "# Create bandit retrieval system with pre-computed embeddings\n",
    "llm = LLMInterface(delay=0.0)  # For simulation\n",
    "bandit = BanditRetrieval(\n",
    "    questions=question_texts,\n",
    "    passages=passage_texts,\n",
    "    query_encoder='sentence-transformer',  # This will be ignored when using pre-computed embeddings\n",
    "    passage_encoder='sentence-transformer',  # This will be ignored when using pre-computed embeddings\n",
    "    beta=2.0,\n",
    "    init_samples=1000,\n",
    "    budget=50,\n",
    "    llm=llm,\n",
    "    question_embeddings=question_embeddings,  # Pass pre-computed embeddings\n",
    "    passage_embeddings=passage_embeddings     # Pass pre-computed embeddings\n",
    ")\n",
    "\n",
    "# Select a few queries for demonstration\n",
    "n_queries = 1\n",
    "selected_query_indices = np.random.choice(len(question_texts), n_queries, replace=False)\n",
    "selected_query_ids = [question_ids[idx] for idx in selected_query_indices]\n",
    "\n",
    "# Store results for evaluation\n",
    "results = []\n",
    "metrics_list = []\n",
    "\n",
    "for i, (query_idx, query_id) in enumerate(zip(selected_query_indices, selected_query_ids)):\n",
    "    print(f\"\\nProcessing query {i+1}/{n_queries}: {question_texts[query_idx][:100]}...\")\n",
    "    \n",
    "    # Run bandit retrieval\n",
    "    ranked_indices, scores, means_history, stds_history = bandit.retrieve(query_idx=query_idx, query_id=query_id, relevance_map=relevance_map,verbose=True,return_trajectories=True)\n",
    "    \n",
    "    # Store results\n",
    "    results.append((query_idx, query_id, ranked_indices, scores, means_history, stds_history))\n",
    "    \n",
    "    # Evaluate if ground truth is available\n",
    "    if query_id in relevance_map:\n",
    "        true_relevance = relevance_map[query_id]\n",
    "        metrics = evaluate_retrieval(ranked_indices, true_relevance)\n",
    "        metrics_list.append(metrics)\n",
    "        print(\"Metrics:\", {k: f\"{v:.3f}\" for k, v in metrics.items()})\n",
    "        \n",
    "    # Display top 3 retrieved passages\n",
    "    print(\"\\nTop retrieved passages:\")\n",
    "    for j in range(min(3, len(ranked_indices))):\n",
    "        passage_idx = ranked_indices[j]\n",
    "        score = scores[passage_idx]\n",
    "        print(f\"{j+1}. Score: {score:.3f}\")\n",
    "        print(f\"   Passage: {passage_texts[passage_idx][:200]}...\")\n",
    "    \n",
    "    # Visualize GP-UCB trajectory\n",
    "    visualize_gp_ucb_trajectory(\n",
    "        means_history, \n",
    "        stds_history, \n",
    "        question_texts[query_idx],\n",
    "        top_passages=5\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "# Plot overall metrics if we have evaluations\n",
    "if metrics_list:\n",
    "    fig, avg_metrics = plot_metrics(metrics_list, selected_query_ids)\n",
    "    plt.show()\n",
    "    print(\"Average metrics:\", {k: f\"{v:.3f}\" for k, v in avg_metrics.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_baseline(questions, passages, query_idx, budget, llm, relevance_map=None, query_id=None):\n",
    "    \"\"\"\n",
    "    Baseline that randomly samples passages.\n",
    "    \n",
    "    Args:\n",
    "        questions: List of question texts\n",
    "        passages: List of passage texts\n",
    "        query_idx: Index of the query\n",
    "        budget: Number of passages to sample\n",
    "        llm: LLM interface for retrieving relevance scores\n",
    "        relevance_map: Ground truth relevance map\n",
    "        query_id: ID of the query in the relevance map\n",
    "        \n",
    "    Returns:\n",
    "        ranked_indices: List of passage indices ranked by relevance\n",
    "        scores: Dictionary mapping passage indices to relevance scores\n",
    "    \"\"\"\n",
    "    query = questions[query_idx]\n",
    "    n_passages = len(passages)\n",
    "    \n",
    "    # Randomly sample passages\n",
    "    sampled_indices = np.random.choice(n_passages, min(budget, n_passages), replace=False)\n",
    "    \n",
    "    # Get relevance scores for sampled passages\n",
    "    scores = {}\n",
    "    for idx in sampled_indices:\n",
    "        passage = passages[idx]\n",
    "        if relevance_map and query_id:\n",
    "            score = llm.get_relevance_score(\n",
    "                query, passage, relevance_map=relevance_map, \n",
    "                query_id=query_id, passage_id=idx\n",
    "            )\n",
    "        else:\n",
    "            score = llm.get_relevance_score(query, passage)\n",
    "        scores[idx] = score\n",
    "    \n",
    "    # Rank the sampled passages by their scores\n",
    "    ranked_indices = sorted(scores.keys(), key=lambda idx: -scores[idx])\n",
    "    \n",
    "    # For unobserved passages, assign a default score\n",
    "    all_scores = np.zeros(n_passages)\n",
    "    for idx, score in scores.items():\n",
    "        all_scores[idx] = score\n",
    "    \n",
    "    return ranked_indices, all_scores\n",
    "\n",
    "def tfidf_baseline(questions, passages, query_idx, budget, llm, top_k=100, relevance_map=None, query_id=None):\n",
    "    \"\"\"\n",
    "    Baseline that uses TF-IDF to pre-rank passages and then scores the top-k.\n",
    "    \n",
    "    Args:\n",
    "        questions: List of question texts\n",
    "        passages: List of passage texts\n",
    "        query_idx: Index of the query\n",
    "        budget: Number of passages to score with LLM\n",
    "        llm: LLM interface for retrieving relevance scores\n",
    "        top_k: Number of top TF-IDF passages to consider\n",
    "        relevance_map: Ground truth relevance map\n",
    "        query_id: ID of the query in the relevance map\n",
    "        \n",
    "    Returns:\n",
    "        ranked_indices: List of passage indices ranked by relevance\n",
    "        scores: Array of relevance scores\n",
    "    \"\"\"\n",
    "    query = questions[query_idx]\n",
    "    n_passages = len(passages)\n",
    "    \n",
    "    # Use TF-IDF to calculate initial similarity scores\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    all_texts = [query] + passages\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "    \n",
    "    # Calculate similarity between query and all passages\n",
    "    query_vec = tfidf_matrix[0:1]\n",
    "    passage_vecs = tfidf_matrix[1:]\n",
    "    similarities = cosine_similarity(query_vec, passage_vecs)[0]\n",
    "    \n",
    "    # Get top-k passages by TF-IDF similarity\n",
    "    top_indices = np.argsort(-similarities)[:top_k]\n",
    "    \n",
    "    # Score top passages with LLM up to budget\n",
    "    llm_indices = top_indices[:min(budget, len(top_indices))]\n",
    "    \n",
    "    scores = {}\n",
    "    for idx in llm_indices:\n",
    "        passage = passages[idx]\n",
    "        if relevance_map and query_id:\n",
    "            score = llm.get_relevance_score(\n",
    "                query, passage, relevance_map=relevance_map, \n",
    "                query_id=query_id, passage_id=idx\n",
    "            )\n",
    "        else:\n",
    "            score = llm.get_relevance_score(query, passage)\n",
    "        scores[idx] = score\n",
    "    \n",
    "    # Re-rank the top passages by LLM scores\n",
    "    ranked_indices = sorted(scores.keys(), key=lambda idx: -scores[idx])\n",
    "    \n",
    "    # For all passages, assign either the LLM score or the normalized TF-IDF score\n",
    "    all_scores = np.zeros(n_passages)\n",
    "    \n",
    "    # For passages not scored by LLM, use normalized TF-IDF scores\n",
    "    norm_similarities = (similarities - np.min(similarities)) / (np.max(similarities) - np.min(similarities))\n",
    "    for i in range(n_passages):\n",
    "        if i in scores:\n",
    "            all_scores[i] = scores[i]  # Use LLM score if available\n",
    "        else:\n",
    "            all_scores[i] = norm_similarities[i] * 0.5  # Scale TF-IDF scores to [0, 0.5]\n",
    "    \n",
    "    return list(ranked_indices) + [i for i in range(n_passages) if i not in ranked_indices], all_scores\n",
    "\n",
    "# Compare methods on a single query\n",
    "query_idx = selected_query_indices[0]\n",
    "query_id = selected_query_ids[0]\n",
    "budget = 50\n",
    "\n",
    "print(f\"Query: {question_texts[query_idx]}\")\n",
    "\n",
    "# GP-UCB (already run above, just re-use the first result)\n",
    "gp_ucb_indices, gp_ucb_scores = results[0][2], results[0][3]\n",
    "\n",
    "# Random baseline\n",
    "random_indices, random_scores = random_baseline(\n",
    "    question_texts, passage_texts, query_idx, budget, llm, \n",
    "    relevance_map=relevance_map, query_id=query_id\n",
    ")\n",
    "\n",
    "# TF-IDF baseline\n",
    "tfidf_indices, tfidf_scores = tfidf_baseline(\n",
    "    question_texts, passage_texts, query_idx, budget, llm, top_k=100,\n",
    "    relevance_map=relevance_map, query_id=query_id\n",
    ")\n",
    "\n",
    "# Evaluate methods\n",
    "if query_id in relevance_map:\n",
    "    true_relevance = relevance_map[query_id]\n",
    "    gp_ucb_metrics = evaluate_retrieval(gp_ucb_indices, true_relevance)\n",
    "    random_metrics = evaluate_retrieval(random_indices, true_relevance)\n",
    "    tfidf_metrics = evaluate_retrieval(tfidf_indices, true_relevance)\n",
    "    \n",
    "    print(\"\\nGP-UCB Metrics:\", {k: f\"{v:.3f}\" for k, v in gp_ucb_metrics.items()})\n",
    "    print(\"Random Baseline Metrics:\", {k: f\"{v:.3f}\" for k, v in random_metrics.items()})\n",
    "    print(\"TF-IDF Baseline Metrics:\", {k: f\"{v:.3f}\" for k, v in tfidf_metrics.items()})\n",
    "    \n",
    "    # Compare methods with bar chart\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    metrics_names = sorted([k for k in gp_ucb_metrics.keys() if k.startswith('nDCG')])\n",
    "    methods = ['GP-UCB', 'Random', 'TF-IDF']\n",
    "    \n",
    "    data = {\n",
    "        'Method': [],\n",
    "        'Metric': [],\n",
    "        'Value': []\n",
    "    }\n",
    "    \n",
    "    for metric in metrics_names:\n",
    "        data['Method'].extend(methods)\n",
    "        data['Metric'].extend([metric] * 3)\n",
    "        data['Value'].extend([\n",
    "            gp_ucb_metrics[metric],\n",
    "            random_metrics[metric],\n",
    "            tfidf_metrics[metric]\n",
    "        ])\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='Metric', y='Value', hue='Method', data=df)\n",
    "    plt.title(f'Comparison of Retrieval Methods (Query: {question_texts[query_idx][:50]}...)')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
